---
layout: default
---

## Updated on 2025.05.24

## Autonomous_Driving_Planning

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-22**|**SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving**|Xuesong Chen et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16805v1-b31b1b.svg)](http://arxiv.org/abs/2505.16805v1)|null|
|**2025-05-22**|**CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving**|Huitong Yang et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16524v1-b31b1b.svg)](http://arxiv.org/abs/2505.16524v1)|null|
|**2025-05-22**|**Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models**|Augusto Luis Ballardini et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16498v1-b31b1b.svg)](http://arxiv.org/abs/2505.16498v1)|null|
|**2025-05-22**|**Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)**|Zhenjie Yang et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16394v1-b31b1b.svg)](http://arxiv.org/abs/2505.16394v1)|null|
|**2025-05-22**|**VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving**|Yansong Qu et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16377v1-b31b1b.svg)](http://arxiv.org/abs/2505.16377v1)|null|
|**2025-05-21**|**VERDI: VLM-Embedded Reasoning for Autonomous Driving**|Bowen Feng et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15925v1-b31b1b.svg)](http://arxiv.org/abs/2505.15925v1)|null|
|**2025-05-21**|**Generative AI for Autonomous Driving: A Review**|Katharina Winter et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15863v1-b31b1b.svg)](http://arxiv.org/abs/2505.15863v1)|null|
|**2025-05-22**|**HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving**|Zhiwen Chen et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15793v2-b31b1b.svg)](http://arxiv.org/abs/2505.15793v2)|null|
|**2025-05-21**|**ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving**|Yunsheng Ma et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15158v1-b31b1b.svg)](http://arxiv.org/abs/2505.15158v1)|null|
|**2025-05-21**|**iPad: Iterative Proposal-centric End-to-End Autonomous Driving**|Ke Guo et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15111v1-b31b1b.svg)](http://arxiv.org/abs/2505.15111v1)|**[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/Kguo-cs/iPad)**|

## Autonomous_Driving_Prediction

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-22**|**Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation**|Moru Liu et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16985v1-b31b1b.svg)](http://arxiv.org/abs/2505.16985v1)|null|
|**2025-05-22**|**NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification**|NovelSeek Team et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16938v1-b31b1b.svg)](http://arxiv.org/abs/2505.16938v1)|null|
|**2025-05-22**|**SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving**|Xuesong Chen et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16805v1-b31b1b.svg)](http://arxiv.org/abs/2505.16805v1)|null|
|**2025-05-22**|**CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving**|Huitong Yang et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16524v1-b31b1b.svg)](http://arxiv.org/abs/2505.16524v1)|null|
|**2025-05-21**|**VERDI: VLM-Embedded Reasoning for Autonomous Driving**|Bowen Feng et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15925v1-b31b1b.svg)](http://arxiv.org/abs/2505.15925v1)|null|
|**2025-05-21**|**HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context Understanding and Future Motion Representation Learning**|Xiaodong Mei et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15703v1-b31b1b.svg)](http://arxiv.org/abs/2505.15703v1)|null|
|**2025-05-21**|**ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving**|Yunsheng Ma et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15158v1-b31b1b.svg)](http://arxiv.org/abs/2505.15158v1)|null|
|**2025-05-21**|**iPad: Iterative Proposal-centric End-to-End Autonomous Driving**|Ke Guo et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15111v1-b31b1b.svg)](http://arxiv.org/abs/2505.15111v1)|**[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/Kguo-cs/iPad)**|
|**2025-05-20**|**Programmatic Video Prediction Using Large Language Models**|Hao Tang et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.14948v1-b31b1b.svg)](http://arxiv.org/abs/2505.14948v1)|**[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/metro-smiles/ProgGen)**|
|**2025-05-20**|**Explaining Unreliable Perception in Automated Driving: A Fuzzy-based Monitoring Approach**|Aniket Salvi et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.14407v1-b31b1b.svg)](http://arxiv.org/abs/2505.14407v1)|null|

## Autonomous_Driving_Decision

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-22**|**SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving**|Xuesong Chen et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16805v1-b31b1b.svg)](http://arxiv.org/abs/2505.16805v1)|null|
|**2025-05-22**|**Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models**|Augusto Luis Ballardini et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16498v1-b31b1b.svg)](http://arxiv.org/abs/2505.16498v1)|null|
|**2025-05-21**|**VERDI: VLM-Embedded Reasoning for Autonomous Driving**|Bowen Feng et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15925v1-b31b1b.svg)](http://arxiv.org/abs/2505.15925v1)|null|
|**2025-05-21**|**Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control**|Seongmin Park et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15304v1-b31b1b.svg)](http://arxiv.org/abs/2505.15304v1)|null|
|**2025-05-21**|**ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving**|Yunsheng Ma et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15158v1-b31b1b.svg)](http://arxiv.org/abs/2505.15158v1)|null|
|**2025-05-20**|**Looking for an out: Affordances, uncertainty and collision avoidance behavior of human drivers**|Leif Johnson et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.14842v1-b31b1b.svg)](http://arxiv.org/abs/2505.14842v1)|null|
|**2025-05-19**|**TS-VLM: Text-Guided SoftSort Pooling for Vision-Language Models in Multi-View Driving Reasoning**|Lihong Chen et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.12670v1-b31b1b.svg)](http://arxiv.org/abs/2505.12670v1)|null|

## Autonomous_Driving_E2E

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-22**|**NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification**|NovelSeek Team et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16938v1-b31b1b.svg)](http://arxiv.org/abs/2505.16938v1)|null|
|**2025-05-22**|**SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving**|Xuesong Chen et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16805v1-b31b1b.svg)](http://arxiv.org/abs/2505.16805v1)|null|
|**2025-05-22**|**CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving**|Huitong Yang et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16524v1-b31b1b.svg)](http://arxiv.org/abs/2505.16524v1)|null|
|**2025-05-22**|**Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)**|Zhenjie Yang et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16394v1-b31b1b.svg)](http://arxiv.org/abs/2505.16394v1)|null|
|**2025-05-22**|**DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving**|Zhenjie Yang et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16278v1-b31b1b.svg)](http://arxiv.org/abs/2505.16278v1)|null|
|**2025-05-21**|**VERDI: VLM-Embedded Reasoning for Autonomous Driving**|Bowen Feng et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15925v1-b31b1b.svg)](http://arxiv.org/abs/2505.15925v1)|null|
|**2025-05-21**|**Challenger: Affordable Adversarial Driving Video Generation**|Zhiyuan Xu et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15880v1-b31b1b.svg)](http://arxiv.org/abs/2505.15880v1)|null|
|**2025-05-21**|**Learning-based Autonomous Oversteer Control and Collision Avoidance**|Seokjun Lee et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15275v1-b31b1b.svg)](http://arxiv.org/abs/2505.15275v1)|null|
|**2025-05-21**|**ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving**|Yunsheng Ma et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15158v1-b31b1b.svg)](http://arxiv.org/abs/2505.15158v1)|null|
|**2025-05-21**|**iPad: Iterative Proposal-centric End-to-End Autonomous Driving**|Ke Guo et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15111v1-b31b1b.svg)](http://arxiv.org/abs/2505.15111v1)|**[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/Kguo-cs/iPad)**|

## Autonomous_Driving_LLM

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-22**|**Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models**|Augusto Luis Ballardini et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16498v1-b31b1b.svg)](http://arxiv.org/abs/2505.16498v1)|null|
|**2025-05-22**|**DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving**|Zhenjie Yang et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16278v1-b31b1b.svg)](http://arxiv.org/abs/2505.16278v1)|null|
|**2025-05-22**|**HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving**|Zhiwen Chen et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15793v2-b31b1b.svg)](http://arxiv.org/abs/2505.15793v2)|null|
|**2025-05-21**|**LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV Cache and Retrieval**|Zhenyu Ning et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15269v1-b31b1b.svg)](http://arxiv.org/abs/2505.15269v1)|null|
|**2025-05-21**|**ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving**|Yunsheng Ma et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15158v1-b31b1b.svg)](http://arxiv.org/abs/2505.15158v1)|null|
|**2025-05-20**|**Programmatic Video Prediction Using Large Language Models**|Hao Tang et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.14948v1-b31b1b.svg)](http://arxiv.org/abs/2505.14948v1)|**[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/metro-smiles/ProgGen)**|
|**2025-05-18**|**A Survey of Attacks on Large Language Models**|Wenrui Xu et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.12567v1-b31b1b.svg)](http://arxiv.org/abs/2505.12567v1)|null|

## Autonomous_Driving_RL

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-22**|**Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)**|Zhenjie Yang et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16394v1-b31b1b.svg)](http://arxiv.org/abs/2505.16394v1)|null|
|**2025-05-22**|**VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving**|Yansong Qu et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16377v1-b31b1b.svg)](http://arxiv.org/abs/2505.16377v1)|null|
|**2025-05-22**|**HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving**|Zhiwen Chen et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15793v2-b31b1b.svg)](http://arxiv.org/abs/2505.15793v2)|null|
|**2025-05-21**|**Learning-based Autonomous Oversteer Control and Collision Avoidance**|Seokjun Lee et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15275v1-b31b1b.svg)](http://arxiv.org/abs/2505.15275v1)|null|
|**2025-05-19**|**Origin-Destination Pattern Effects on Large-Scale Mixed Traffic Control via Multi-Agent Reinforcement Learning**|Muyang Fan et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.13543v1-b31b1b.svg)](http://arxiv.org/abs/2505.13543v1)|null|
|**2025-05-18**|**Distributional Soft Actor-Critic with Harmonic Gradient for Safe and Efficient Autonomous Driving in Multi-lane Scenarios**|Feihong Zhang et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.13532v1-b31b1b.svg)](http://arxiv.org/abs/2505.13532v1)|null|

## World_Model

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2025-05-22**|**Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation**|Moru Liu et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16985v1-b31b1b.svg)](http://arxiv.org/abs/2505.16985v1)|null|
|**2025-05-22**|**Human-like Semantic Navigation for Autonomous Driving using Knowledge Representation and Large Language Models**|Augusto Luis Ballardini et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16498v1-b31b1b.svg)](http://arxiv.org/abs/2505.16498v1)|null|
|**2025-05-22**|**Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)**|Zhenjie Yang et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16394v1-b31b1b.svg)](http://arxiv.org/abs/2505.16394v1)|null|
|**2025-05-22**|**VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving**|Yansong Qu et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16377v1-b31b1b.svg)](http://arxiv.org/abs/2505.16377v1)|null|
|**2025-05-22**|**BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World**|Ji Guo et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.16154v1-b31b1b.svg)](http://arxiv.org/abs/2505.16154v1)|null|
|**2025-05-21**|**VERDI: VLM-Embedded Reasoning for Autonomous Driving**|Bowen Feng et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15925v1-b31b1b.svg)](http://arxiv.org/abs/2505.15925v1)|null|
|**2025-05-21**|**RIS Beam Calibration for ISAC Systems: Modeling and Performance Analysis**|Mengting Li et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15403v1-b31b1b.svg)](http://arxiv.org/abs/2505.15403v1)|null|
|**2025-05-21**|**Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control**|Seongmin Park et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15304v1-b31b1b.svg)](http://arxiv.org/abs/2505.15304v1)|null|
|**2025-05-22**|**AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving**|Kangan Qian et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15298v2-b31b1b.svg)](http://arxiv.org/abs/2505.15298v2)|null|
|**2025-05-21**|**LiveVLM: Efficient Online Video Understanding via Streaming-Oriented KV Cache and Retrieval**|Zhenyu Ning et.al.|[![arxiv](https://img.shields.io/badge/arXiv-2505.15269v1-b31b1b.svg)](http://arxiv.org/abs/2505.15269v1)|null|

